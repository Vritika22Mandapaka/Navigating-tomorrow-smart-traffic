# -*- coding: utf-8 -*-
"""Navigating_tmrwobjdect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HAPAexikyjdwwD3MBZsf6ta4Bya8PTzq
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import ImageFolder
from torch.autograd import Variable
import cv2
from google.colab.patches import cv2_imshow


# Define a custom dataset class
class CustomDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data = ImageFolder(root=data_dir, transform=transform)  # Use the provided path here

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# Set your dataset directory
data_dir = '/content/train'  # Path is used here
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

class_labels = labels.id2label

import sys
import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests
import threading

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://a98c-2409-408c-1dcc-a6b8-28f1-37b9-e030-3f06.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    try:
        requests.post(url, data={'command': command})
        print(f'Successfully sent command: {command}')
    except Exception as e:
        print(f'Failed to send command. Error: {str(e)}')

# Function to capture an image
def capture_image():
    filename = 'photo.jpg'
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(0.8))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

try:
    # Capture an image
    filename = capture_image()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Display the resulting frame
    display(IPImage(filename=filename))

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Send vehicle type to local server
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency

    # Use threading for both cases
    threading.Thread(target=send_command, args=(vehicle_type,)).start()
    print(f'Signal sent to local server for {label_name}.')

    # Display result on the frame
    print(f'Prediction: {label_name}')

finally:
    # Close any open connections or resources here
    print("Closing connections or resources.")