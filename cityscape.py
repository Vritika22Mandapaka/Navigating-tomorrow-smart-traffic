# -*- coding: utf-8 -*-
"""Cityscape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tRMQUhjfd47IGfnq-q2BJBlU5nIsrd9E
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import ImageFolder
from torch.autograd import Variable
import cv2
from google.colab.patches import cv2_imshow


# Define a custom dataset class
class CustomDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data = ImageFolder(root=data_dir, transform=transform)  # Use the provided path here

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# Set your dataset directory
data_dir = '/content/train'  # Path is used here
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])



!python -m pip install cityscapesscripts

!python -m pip install cityscapesscripts

!python -m pip install cityscapesscripts[gui]

import cityscapesscripts.helpers.labels as labels

model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

class_labels = labels.id2label

!git clone https://github.com/mcordts/cityscapesScripts

import shutil
sorce = "/content/cityscapesScripts"
destination="/content/drive/MyDrive/CityScape"
shutil.copytree(sorce,destination,dirs_exist_ok=True)

import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
import cityscapesscripts.helpers.labels as labels

# 1. Install or check for required libraries
# !pip install cityscapesscripts  # If not already installed

# 2. Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# 3. Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# 4. Get class labels from Cityscapes
class_labels = labels.id2label

# 5. Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# 6. Main execution with error-checking
try:
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Display result on the frame
    print(f'Prediction: {label_name}')

    # Display the resulting frame
    display(IPImage(filename=filename))

except Exception as err:
    print("Error encountered:", str(err))

finally:
    # Release resources
    cv2.destroyAllWindows()

import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode

# 1. Install or check for required libraries
# !pip install cityscapesscripts  # If not already installed

# 2. Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# 3. Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# 4. Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}

# 5. Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# 6. Main execution with error-checking
try:
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Display result on the frame
    print(f'Prediction: {label_name}')

    # Display the resulting frame
    display(IPImage(filename=filename))

except Exception as err:
    print("Error encountered:", str(err))

finally:
    # Release resources
    cv2.destroyAllWindows()

import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode

# 1. Install or check for required libraries
# !pip install cityscapesscripts  # If not already installed

# 2. Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# 3. Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# 4. Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

# 5. Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# 6. Main execution with error-checking
try:
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Display result on the frame
    print(f'Prediction: {label_name}')

    # Display the resulting frame
    display(IPImage(filename=filename))

except Exception as err:
    print("Error encountered:", str(err))

finally:
    # Release resources
    cv2.destroyAllWindows()

!pip install pyserial

!pip install serial
import serial

import serial
arduino = serial.Serial('Arduino Uno (COM3)', 9600)  # Replace with the actual port



import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
import serial

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

# Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Initialize arduino to None
arduino = None

# Main execution with error-checking
try:
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Send vehicle type to Arduino
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
        print ("E")
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency
        print("N")

    # Connect to Arduino
    arduino = serial.Serial('com3', 9600)  # Adjust port if needed
    arduino.write(vehicle_type.encode())

    # Display result on the frame
    print(f'Prediction: {label_name}')

    # Display the resulting frame
    display(IPImage(filename=filename))

except Exception as err:
    print("Error encountered:", str(err))

finally:
    # Release resources
    cv2.destroyAllWindows()
    if arduino is not None:
        arduino.close()  # Close serial connection

# NOte 2 lo error vaste edhi paste cheyu -> model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
#model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
#model.eval()

model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

"""**With Arduino and NGROK**

"""

import sys
import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://999e-2405-201-c01d-f0fe-4861-61e3-e358-c55c.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    try:
        requests.post(url, data={'command': command})
        print(f'Successfully sent command: {command}')
    except Exception as e:
        print(f'Failed to send command. Error: {str(e)}')

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

# Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

try:
    # Capture an image
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Display result on the frame
    print(f'Prediction: {label_name}')

    # Display the resulting frame
    display(IPImage(filename=filename))

    # Send vehicle type to local server
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
        try:
            # Send the command to the local server
            send_command(vehicle_type)
            print("Signal sent to local server for Emergency Vehicle.")
        except Exception as server_err:
            print("Error communicating with the local server:", str(server_err))
            sys.exit()  # Stop execution
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency

finally:
    # Close any open connections or resources here
    print("Closing connections or resources.")

import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://5bbc-2405-201-c01d-f0fe-147b-1390-ffeb-f3ba.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    response = requests.post(url, data={'command': command})

    if response.status_code == 200:
        print(f'Successfully sent command: {command}')
    else:
        print(f'Failed to send command. Status code: {response.status_code}')

# Example usage:
send_command('E')  # Sending 'E' as the command


# Function to send a command to the local server
#def send_command(command):
   # client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   # client_socket.connect(('9101-2405-201-c01d-f0fe-a9ee-b956-b1a4-dadf.ngrok-free.app ', 80))  # Use your ngrok server address and port
   # client_socket.send(command.encode())
   # client_socket.close()

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

# Define the take_photo function
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(quality))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Capture an image
filename = take_photo()
print('Saved to {}'.format(filename))

# Load the captured image as a PIL Image
pil_image = Image.open(filename)

# Preprocess the image
frame_tensor = transform(pil_image).unsqueeze(0)
if frame_tensor is None:  # Check for valid preprocessing output
    raise ValueError("Preprocessing produced a None output!")

# Make prediction
with torch.no_grad():
    output = model(frame_tensor)
    if output is None:  # Check for valid model output
        raise ValueError("Model returned a None output!")

# Extract predicted class
_, predicted = torch.max(output.data, 1)
label_id = predicted.item()
label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

# Send vehicle type to local server
if label_name == "Emergency Vehicle":
    vehicle_type = 'E'  # Send 'E' for emergency
    try:
        # Send the command to the local server
        send_command(vehicle_type)
        print("Signal sent to local server for Emergency Vehicle.")
    except Exception as server_err:
        print("Error communicating with the local server:", str(server_err))
else:
    vehicle_type = 'N'  # Send 'N' for non-emergency

# Display result on the frame
print(f'Prediction: {label_name}')

# Display the resulting frame
display(IPImage(filename=filename))

import sys
import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests
import threading

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://5bbc-2405-201-c01d-f0fe-147b-1390-ffeb-f3ba.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    try:
        requests.post(url, data={'command': command})
        print(f'Successfully sent command: {command}')
    except Exception as e:
        print(f'Failed to send command. Error: {str(e)}')

# Function to capture an image
def capture_image():
    filename = 'photo.jpg'
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(0.8))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

try:
    # Capture an image
    filename = capture_image()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Display the resulting frame
    display(IPImage(filename=filename))

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Send vehicle type to local server
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
        threading.Thread(target=send_command, args=(vehicle_type,)).start()  # Use threading
        print("Signal sent to local server for Emergency Vehicle.")
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency

    # Display result on the frame
    print(f'Prediction: {label_name}')

finally:
    # Close any open connections or resources here
    print("Closing connections or resources.")

"""**Main**"""

model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

import sys
import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests
import threading

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://999e-2405-201-c01d-f0fe-4861-61e3-e358-c55c.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    try:
        requests.post(url, data={'command': command})
        print(f'Successfully sent command: {command}')
    except Exception as e:
        print(f'Failed to send command. Error: {str(e)}')

# Function to capture an image
def capture_image():
    filename = 'photo.jpg'
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(0.8))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

try:
    # Capture an image
    filename = capture_image()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Display the resulting frame
    display(IPImage(filename=filename))

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Send vehicle type to local server
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency

    # Use threading for both cases
    threading.Thread(target=send_command, args=(vehicle_type,)).start()
    print(f'Signal sent to local server for {label_name}.')

    # Display result on the frame
    print(f'Prediction: {label_name}')

finally:
    # Close any open connections or resources here
    print("Closing connections or resources.")

model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')  # Example using a semi-supervised ResNet50
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

import sys
import socket
from IPython.display import display, Javascript, Image as IPImage
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import requests
import threading

# Function to send a command to the local server
def send_command(command):
    ngrok_url = 'https://46d1-2409-408c-2e9e-35c5-901b-83-6e67-d4ab.ngrok-free.app'  # Use your ngrok server address

    url = f'{ngrok_url}'
    try:
        requests.post(url, data={'command': command})
        print(f'Successfully sent command: {command}')
    except Exception as e:
        print(f'Failed to send command. Error: {str(e)}')

# Function to capture an image
def capture_image():
    filename = 'photo.jpg'
    js = Javascript('''
    async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        // Resize the output to fit the video element.
        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        // Wait for the Capture to be clicked.
        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getVideoTracks()[0].stop();
        div.remove();
        return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)

    # Use JavaScript to capture a photo and save it
    data = eval_js('takePhoto({})'.format(0.8))
    binary_data = b64decode(data.split(',')[1])

    # Remove special characters from the filename
    filename = ''.join(c for c in filename if c.isalnum() or c in ['_', '.'])

    with open(filename, 'wb') as f:
        f.write(binary_data)

    return filename

# Load the pre-trained model (adjust path if different)
model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')
model.fc = nn.Linear(2048, 2)  # Modify for 2 classes
model.eval()

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Get class labels from Cityscapes
class_labels = {0: 'Non-Emergency Vehicle', 1: 'Emergency Vehicle'}
emergency_vehicles = ["ambulance", "police", "fire truck"]
non_emergency_vehicles = ["bus", "bicycle", "truck", "car"]

try:
    # Capture an image
    filename = capture_image()
    print('Saved to {}'.format(filename))

    # Load the captured image as a PIL Image
    pil_image = Image.open(filename)

    # Display the resulting frame
    display(IPImage(filename=filename))

    # Preprocess the image
    frame_tensor = transform(pil_image).unsqueeze(0)
    if frame_tensor is None:  # Check for valid preprocessing output
        raise ValueError("Preprocessing produced a None output!")

    # Make prediction
    with torch.no_grad():
        output = model(frame_tensor)
        if output is None:  # Check for valid model output
            raise ValueError("Model returned a None output!")

    # Extract predicted class
    _, predicted = torch.max(output.data, 1)
    label_id = predicted.item()
    label_name = class_labels[label_id] if label_id in class_labels else f'Class {label_id}'

    # Send vehicle type to local server
    if label_name == "Emergency Vehicle":
        vehicle_type = 'E'  # Send 'E' for emergency
    else:
        vehicle_type = 'N'  # Send 'N' for non-emergency

    # Use threading for both cases
    threading.Thread(target=send_command, args=(vehicle_type,)).start()
    print(f'Signal sent to local server for {label_name}.')

    # Display result on the frame
    print(f'Prediction: {label_name}')

finally:
    # Close any open connections or resources here
    print("Closing connections or resources.")

"""**Flutter** **trail**- https://youtu.be/B9hsWOCXb_o?si=umv_ZbgwGVJJc48t"""

